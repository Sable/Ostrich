#!/usr/bin/env node
var path = require('path')
var configLib = require(path.join(__dirname, '../lib/config'))
var nopt = require('nopt')
var noptUsage = require('nopt-usage')
var pointer = require('json-pointer')
var extend = require('extend')
var shelljs = require('shelljs')
var moment = require('moment')
var fs = require('fs')
var crypto = require('crypto')

var knownOpts = {
  'benchmark': [String, Array],
  'build': [String, Array],
  'clean': Boolean,
  'compiler': [String, Array],
  // 'experiment': String,
  'help': Boolean,
  'implementation': [String, Array],
  'input-size': String,
  'iteration-number': Number,
  'platform': String,
  'environment': [String, Array],
  'skip-output-verification': Boolean,
  'verbose': Boolean
}
var shortHands = {
  'b': ['--benchmark'],
  'bu': ['--build'],
  'c': ['--compiler'],
  'en': ['--environment'],
  'e': ['--experiment'],
  'h': ['--help'],
  'i': ['--implementation'],
  'is': ['--input-size'],
  'n': ['--iteration-number'],
  'p': ['--platform'],
  'v': ['--verbose']
}

var suiteRoot = path.join(__dirname, '../../../')

function deepcopy (o) {
  if (Array.prototype.isPrototypeOf(o)) {
    return extend(true, [], o)
  } else {
    return extend(true, {}, o)
  }
}

var runOutputSchemaPath = '/definitions/run'
var validRunOutput = configLib.createMatcher(runOutputSchemaPath)
function executeRun (config, options) {
  var checkOutput = !options['skip-output-verification'] && pointer.has(config, '/benchmark/output/output-check-arguments')
  var environmentRunPath = path.join(config.environment.location, '/run')
  var runnerPath = path.join(config.location, '/' + config.compiler['runner-name'])
  var runnerArguments = checkOutput && config.implementation.hasOwnProperty('runner-arguments-with-output-check')
    ? config.implementation['runner-arguments-with-output-check'].join(' ')
    : config.implementation['runner-arguments'].join(' ')
  var cmd = environmentRunPath + ' ' + runnerPath + ' ' + runnerArguments
  var status = shelljs.exec(cmd, {silent: !options.verbose})
  if (status.code !== 0) {
    console.log("Execution error for '" + cmd + "':")
    console.log(status.output)
    process.exit(1)
  }

  var jsonOutputMatch = status.output.match(/\{.*\}/)
  var jsonOutput
  if (jsonOutputMatch === null) {
    console.log('Invalid output for ' + runnerPath + ' with ' + runnerArguments + ',')
    console.log('missing json result:')
    console.log(status.output)
    process.exit(1)
  }

  try {
    jsonOutput = JSON.parse(jsonOutputMatch)
  } catch (e) {
    console.log('Invalid output for ' + runnerPath + ' with ' + runnerArguments + ',')
    console.log('improperly formatted json result:')
    console.log(status.output)
    process.exit(1)
  }

  if (!validRunOutput(jsonOutput)) {
    console.log('Invalid output for ' + runnerPath + ' with ' + runnerArguments + ',')
    console.log('json object does not conform to ' + runOutputSchemaPath + ' schema:')
    console.log(status.output)
    process.exit(1)
  }

  if (checkOutput) {
    if (options.verbose) {
      process.stdout.write('Verifying output: ')
    }

    if (config.benchmark.output.type === 'output-value') {
      config.experiment['output-value'] = JSON.stringify(jsonOutput.output)
      configLib.expand(config, {strict: true})
    }

    cmd = pointer.get(config, '/benchmark/location') + '/output/check ' +
      pointer.get(config, '/benchmark/output/output-check-arguments').join(' ')
    status = shelljs.exec(cmd, {silent: !options.verbose})
    if (status.code !== 0) {
      console.log("error for '" + cmd + "':")
      console.log(status.output)
      process.exit(1)
    }
    if (options.verbose) {
      process.stdout.write('valid\n')
    }
  }

  return jsonOutput
}

configLib.config(suiteRoot, function (err, config) {
  if (err) {
    console.log(err)
    process.exit(1)
  }

  var description = {
    'benchmark': "Benchmark's short-name, repeat to specify multiple benchmarks",
    'build': 'Build hash value, repeat to specify multiple builds',
    'clean': 'Clean the runs folder',
    'compiler': "Compiler's short-name, repeat to specify multiple compilers",
    'environment': "Environment's short-name, repeat to specify multiple environments",
    'experiment': 'Path to experiment configuration file',
    'help': 'Display this help',
    'implementation': "Implementation's short-name, repeat to specify multiple implementations",
    'input-size': 'One of [' + pointer.get(config.schema, '/definitions/experiment/properties/input-size/enum') + '], defaults to medium',
    'iteration-number': 'Number of times to execute the benchmark',
    'platform': 'Path to a configuration file describing the current hardware configuration',
    'skip-output-verification': 'Do not perform verification of output results'
  }
  var parsed = nopt(knownOpts, shortHands)

  if (parsed.help) {
    var usage = noptUsage(knownOpts, shortHands, description)
    console.log('usage: run [options] [short-name [short-name ...]]\n')
    console.log(
      'Runs builds on different environments. Defaults to all compatible builds\n' +
      'and environments. Specify short-name(s) for benchmarks, implementations,\n' +
      'or compilers to run builds that match at least one name in each category specified.\n')
    console.log('positional arguments: ')
    console.log("    short-name\t\t\t\tbenchmark, implementation, or compiler's short-name. ")
    console.log('')
    console.log('optional arguments:')
    console.log(usage)
    process.exit(1)
  }

  if (parsed.clean) {
    shelljs.rm('-rf', path.join(suiteRoot, '/runs'))
    process.exit(0)
  }

  var options = {
    'platform': JSON.parse(fs.readFileSync(parsed.hasOwnProperty('platform') ? parsed['platform'] : path.join(suiteRoot, '/platforms/mba-2011/platform.json')))
  }

  // Load builds
  var run = {
    results: {},
    platform: options.platform,
    experiment: {
      'type': 'experiment',
      'short-name': 'default',
      'input-size': parsed['input-size'] || 'medium',
      'iteration-number': parsed['iteration-number'] || 1
    }
  }

  // Extract short-name(s) from position arguments

  parsed.benchmarks = parsed.benchmark || []
  parsed.compilers = parsed.compiler || []
  parsed.implementations = parsed.implementation || []
  parsed.environments = parsed.environment || []

  parsed.argv.remain.forEach(function (n) {
    function isAmbiguous () {
      if (matched) {
        console.log("'" + n + "' matches multiple categories, specify its category with a flag.")
        process.exit(1)
      } else {
        matched = true
      }
    }
    var matched = false
    var categories = [
      'benchmark',
      'compiler',
      'implementation',
      'environment'
    ]

    categories.forEach(function (c) {
      if (config[c + '-list'].indexOf(n) > -1) {
        parsed[c + 's'].push(n)
        isAmbiguous()
      }
    })

    if (!matched) {
      console.log("invalid short-name '" + n + "', available short-name(s):")
      categories.forEach(function (c) {
        console.log('  ' + c + 's: ' + config[c + '-list'])
      })
      process.exit(1)
    }
  })

  function printBuildSummary (shortName) {
    var b = config.builds[shortName]
    console.log(
      '    ' + b['short-name'] + ':\n' +
      '        benchmark:      ' + b.benchmark['short-name'] + '\n' +
      '        implementation: ' + b.implementation['short-name'] + '\n' +
      '        compiler:       ' + b.compiler['short-name'] + '\n' +
      '        build time:     ' + b.time)
  }
  function mostRecentBuilds (b0, b1) {
    var m0 = moment(config.builds[b0].time)
    var m1 = moment(config.builds[b1].time)
    if (m0.isSame(m1)) {
      return 0
    } else {
      return m0.isBefore(m1) ? 1 : -1
    }
  }
  function filterCategory (category, names) {
    return function (buildName) {
      var build = config.builds[buildName]
      return names.indexOf(build[category]['short-name']) > -1
    }
  }

  var builds = parsed.build || []
  var buildCategories = [
    'benchmark',
    'compiler',
    'implementation'
  ]
  if (builds.length === 0) {
    builds = Object.keys(config.builds)
    buildCategories.forEach(function (c) {
      var names = parsed[c + 's']
      if (names.length > 0) {
        builds = builds.filter(filterCategory(c, names))
      }
    })
  } else {
    builds = builds.map(function (b) {
      var m = b.match(/(.*builds\/)?([a-zA-Z0-9]+).*/)
      if (m === null) {
        throw new Error('Invalid build path ' + b)
      }
      var p = m[2]
      var matches = Object.keys(config.builds).filter(function (b) {
        return b.match(RegExp('^' + p))
      })

      if (matches.length === 0) {
        console.log("No build with prefix '" + p + "', latest builds:")
        Object.keys(config.builds).sort(mostRecentBuilds).slice(0, 3).forEach(printBuildSummary)
        process.exit(1)
      }

      if (matches.length > 1) {
        console.log("Multiple builds with prefix '" + p + "':")
        matches.sort(mostRecentBuilds).forEach(printBuildSummary)
        process.exit(1)
      }

      return matches[0]
    })
  }
  builds = builds.map(function (name) {
    return pointer.get(config, '/builds/' + name)
  })

  if (builds.length === 0) {
    console.log('no builds to run')
    process.exit(1)
  }

  var environments = (parsed.environments.length === 0
    ? config['environment-list']
    : parsed.environments)
    .map(function (name) {
      return config.environments[name]
    })

  function valid (compiler, environment) {
    var targetLanguages = compiler['target-languages']
    var supportedLanguages = environment['supported-languages']

    var validLanguage = false
    for (var i = 0; i < targetLanguages.length; ++i) {
      for (var j = 0; j < supportedLanguages.length; ++j) {
        validLanguage = validLanguage || (targetLanguages[i] === supportedLanguages[j])
      }
    }
    return validLanguage
  }

  builds.map(deepcopy).forEach(function (runConfig) {
    environments.forEach(function (env) {
      if (!valid(runConfig.compiler, env)) {
        return
      }

      runConfig.experiment = {
        'type': 'experiment',
        'input-size': run.experiment['input-size'],
        'input-file': [{'expand': '/experiment/input-size'}, {'config': '/benchmark/random-seed'}],
        'output-file': pointer.has(runConfig, '/benchmark/output/output-file')
          ? { 'config': '/benchmark/output/output-file' } : { 'file': './output.csv' }
      }
      runConfig.environment = env
      // Expand run-time arguments
      configLib.expand(runConfig)

      // console.log(runConfig)

      var result = {
        build: runConfig,
        experiment: run.experiment,
        platform: run.platform,
        environment: env,
        'times': []
      }

      console.log('-------------- ' + runConfig.benchmark['short-name'] +
        ',' + runConfig.implementation['short-name'] +
        ',' + runConfig.compiler['short-name'] +
        ',' + run.platform['short-name'] +
        ',' + env['short-name'] +
        ' --------------')

      for (var i = 0; i < run.experiment['iteration-number']; ++i) {
        if (parsed.verbose) {
          console.log('************** Iteration ' + i + ' ************')
        }
        var output = executeRun(runConfig, parsed)
        result.times.push(output.time)
      }

      var hash = crypto.createHash('sha1')
      hash.update(JSON.stringify({
        build: result.build,
        experiment: runConfig.experiment,
        platform: result.platform
      }))
      run.results[hash.digest('hex')] = result
    })
  })

  var silentState = shelljs.config.silent
  shelljs.config.silent = true
  run.time = moment().format()
  var dir = path.join(suiteRoot, '/runs/', run.time)
  shelljs.mkdir('-p', dir)
  shelljs.ln('-s', dir, path.join(suiteRoot, '/runs/latest'))
  JSON.stringify(run, null, '  ').to(path.join(dir, '/run.json'))
  shelljs.config.silent = silentState
})
